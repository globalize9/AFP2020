{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# # Importing Libraries\n",
    "\n",
    "# In[1]:\n",
    "\n",
    "\n",
    "import random\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime as dt \n",
    "from itertools import combinations\n",
    "import pymannkendall as mk\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import scipy.stats as sp\n",
    "import pickle\n",
    "\n",
    "\n",
    "\n",
    "# # Getting Candidate Pairs\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "all_data_2 = pd.read_excel(\"Final_Data.xlsx\", sheet_name = \"Sheet3\", header = [0,1])\n",
    "# all_data_2 = pd.read_excel(\"Data_Full_Editable.xlsx\", sheet_name = \"Sheet3\", header = [0,1])\n",
    "\n",
    "all_data_2.index = all_data_2['Unnamed: 0_level_0']['Dates']\n",
    "\n",
    "all_data_2_raw = all_data_2.drop(columns ='Unnamed: 0_level_0' )\n",
    "\n",
    "all_data_2 = all_data_2_raw[all_data_2_raw.index <= \"2019-12-31\"] # specify end_date\n",
    "\n",
    "companies = np.array(all_data_2.columns.get_level_values(0).unique()) #All the unique companies\n",
    "\n",
    "all_combinations = []\n",
    "for i in combinations(companies,2):\n",
    "    all_combinations.append((i[0]+ \"_\" + i[1]))\n",
    "\n",
    "all_combinations\n",
    "\n",
    "\n",
    "# In[3]:\n",
    "\n",
    "\n",
    "candidate_pairs_overbought = pd.DataFrame({'Return_to_trend':[], 'Trend_adj_peak_trough':[],\n",
    "                                           'SD_moves':[], 'Num_above_trend':[], 'Num_below_trend':[], 'Num_above_200MA':[], 'Num_below_200MA':[], 'peaking':[], 'troughing':[]})\n",
    "candidate_pairs_candy_fish = candidate_pairs_overbought.copy()\n",
    "candidate_pairs_rolled_over = candidate_pairs_overbought.copy()\n",
    "candidate_pairs_crossed_over = candidate_pairs_overbought.copy()\n",
    "candidate_pairs_put_call = candidate_pairs_overbought.copy()\n",
    "\n",
    "def get_RSI_14day(data):\n",
    "    df_1_new = data.reset_index().copy()\n",
    "    df_1_new.loc[:,'avg_14up'] = 0\n",
    "    df_1_new.loc[:,'avg_14down'] = 0\n",
    "    df_1_new.loc[13,'avg_14up'] = df_1_new.loc[range(0,14),'Up'].mean()\n",
    "    df_1_new.loc[13,'avg_14down'] = df_1_new.loc[range(0,14),'Down'].mean()\n",
    "    for i in range(14,df_1_new.shape[0]):\n",
    "        df_1_new.loc[i,'avg_14up'] = (df_1_new.loc[(i-1),'avg_14up']*13 + df_1_new.loc[i,'Up'])/14\n",
    "        df_1_new.loc[i,'avg_14down'] = (df_1_new.loc[(i-1),'avg_14down']*13 + df_1_new.loc[i,'Down'])/14\n",
    "    df_1_new['RS'] = df_1_new['avg_14up']/df_1_new['avg_14down']\n",
    "    df_1_new['RSI'] = (100 - (100/(1+df_1_new['RS'])))\n",
    "    return(np.array(df_1_new['RSI']))\n",
    "\n",
    "all_indicators = pd.DataFrame({'Index':[0],\n",
    "                          'RSI':[0],\n",
    "                          'Std.dev_PE':[0],\n",
    "                          'Avg_PE':[0],\n",
    "                          'PE':[0],\n",
    "                          'Recent_Peak':[0],\n",
    "                          'MoM':[0],\n",
    "                          '3M_MoM':[0],\n",
    "                          'YoY':[0],\n",
    "                          'Max_YoY':[0],\n",
    "                          '50_MA':[0],\n",
    "                          '200_MA':[0],\n",
    "                          '50_day_test_peak':[0],\n",
    "                          '50_day_test_trough':[0],\n",
    "                          'Min_YoY':[0],\n",
    "                          'PC_EWM20':[0]})\n",
    "\n",
    "\n",
    "# In[4]:\n",
    "\n",
    "\n",
    "start_trend = '2015-01-01'\n",
    "\n",
    "def LinearTrend(dataset):\n",
    "    # input data series with dates on the index, i.e. df_1['Index']\n",
    "    dataset_subset = dataset.loc[start_trend:]\n",
    "    y=np.array(dataset_subset.values, dtype=float)\n",
    "    x=np.array(pd.to_datetime(dataset_subset.index.values), dtype=float)\n",
    "    slope, intercept, r_value, p_value, std_err =sp.linregress(x,y)\n",
    "    yf = (slope*x)+intercept    \n",
    "    return pd.DataFrame(yf, columns = ['trend'], index = dataset[start_trend:].index)\n",
    "\n",
    "def TrendAdjPT(dataset, trend_data):\n",
    "    last_peak_loc = df_1_test.index[np.where(df_1_test.Peak==1)[0][-1]]\n",
    "    \n",
    "    trend_factor = trend_data.iloc[len(trend_data)-1] / trend_data[trend_data.index == last_peak_loc] - 1\n",
    "    return trend_factor.values\n",
    "\n",
    "def Days_above_below(dataset, trend_data):    \n",
    "    df_1_test = pd.DataFrame(dataset).copy()\n",
    "    df_1_test.loc[:,'Trend'] = trend_data.loc[:,'trend']\n",
    "    df_1_test.loc[:,'Index_200MA'] = df_1_test.loc[:,'Index'].rolling(window = 200).mean()\n",
    "    df_1_test['Above_Below'] = df_1_test.apply(lambda x: 'Above' if x['Index']>x['Trend'] else 'Below' if x['Index']<x['Trend'] else np.nan, axis = 1)\n",
    "    df_1_test['Above_Below_200MA'] = df_1_test.apply(lambda x: 'Above' if x['Index']>x['Index_200MA'] else 'Below' if x['Index']<x['Index_200MA'] else np.nan, axis = 1)\n",
    "    Num_above_trend = df_1_test[df_1_test['Above_Below']==\"Above\"].shape[0]\n",
    "    Num_below_trend = df_1_test[df_1_test['Above_Below']==\"Below\"].shape[0]\n",
    "    Num_above_200MA = df_1_test[df_1_test['Above_Below_200MA']==\"Above\"].shape[0]\n",
    "    Num_below_200MA = df_1_test[df_1_test['Above_Below_200MA']==\"Below\"].shape[0]\n",
    "    \n",
    "    return Num_above_trend, Num_below_trend, Num_above_200MA, Num_below_200MA\n",
    "\n",
    "def SDmoves(ind_data, trend_data):\n",
    "    diff = abs(ind_data - trend_data.iloc[:,0])\n",
    "    sd_moves = np.std(diff)\n",
    "    return sd_moves\n",
    "\n",
    "def Perc_above_below(dataset, trend_data):\n",
    "    ab_factor = trend_data.iloc[len(trend_data)-1] / dataset.iloc[len(dataset)-1]  - 1\n",
    "    return ab_factor.values\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "start = dt.datetime.now()\n",
    "for i in range(len(all_combinations)):\n",
    "    print(i)\n",
    "    df_1= pd.DataFrame({'Index':[],\n",
    "                        'PE_Ratio':[],\n",
    "                        'Div_Diff':[]})\n",
    "    \n",
    "    ###### calculating the technical indicators\n",
    "    df_1['Index'] = all_data_2[str(all_combinations[i].split('_')[0])]['PX_LAST']/all_data_2[str(all_combinations[i].split('_')[1])]['PX_LAST']\n",
    "    df_1['PE_Ratio'] = all_data_2[str(all_combinations[i].split('_')[0])]['PE_RATIO']/all_data_2[str(all_combinations[i].split('_')[1])]['PE_RATIO']\n",
    "    df_1['Div_Diff'] = (all_data_2[str(all_combinations[i].split('_')[0])]['AVERAGE_DIVIDEND_YIELD'] - all_data_2[str(all_combinations[i].split('_')[1])]['AVERAGE_DIVIDEND_YIELD'])\n",
    "\n",
    "    if df_1['Index'].isna().all(): continue \n",
    "    \n",
    "    df_1['Return'] = df_1.Index.pct_change()\n",
    "    df_1['Change'] = df_1.Index.diff()\n",
    "    \n",
    "    df_1['1Y_lag_Index'] = df_1['Index'].shift(252)\n",
    "    df_1['YoY_change'] = df_1['Index']/df_1['1Y_lag_Index'] - 1\n",
    "    \n",
    "    df_1['1M_lag_Index'] = df_1['Index'].shift(21)\n",
    "    df_1['MoM_change'] = df_1['Index']/df_1['1M_lag_Index'] - 1\n",
    "    \n",
    "    df_1['200_MA'] = df_1['Index'].rolling(window = 200).mean()\n",
    "    df_1['50_MA'] = df_1['Index'].rolling(window = 50).mean()\n",
    "    \n",
    "    df_1['Up'] = df_1['Change']\n",
    "    df_1.loc[(df_1['Up']<0), 'Up'] = 0\n",
    "    \n",
    "    df_1['Down'] = df_1['Change']\n",
    "    df_1.loc[(df_1['Down']>0), 'Down'] = 0 \n",
    "    df_1['Down'] = abs(df_1['Down']) # taking the absolute value for downward movements\n",
    "    \n",
    "    df_1['200Day_diff'] = df_1['Index']/df_1['200_MA'] - 1\n",
    "    \n",
    "    \n",
    "    df_1 = df_1.dropna(subset = ['Index'])\n",
    "    df_1['RSI'] = get_RSI_14day(df_1)\n",
    "    \n",
    "    \n",
    "    # start = dt.datetime.strptime('2010-01-01', '%Y-%m-%d')\n",
    "    end = dt.datetime.strptime('2019-12-31', '%Y-%m-%d')\n",
    "    df_1_test = df_1.loc[:end,]\n",
    "    \n",
    "    df_1_test.loc[:,'20_day_EWM'] = df_1_test['Index'].ewm(span = 20).mean()\n",
    "    df_1_test.loc[:,'First_Derivative'] = (df_1_test['20_day_EWM'].shift(1) - df_1_test['20_day_EWM'].shift(-1))/2\n",
    "    df_1_test.loc[:,'Lag_First_Derivative'] = df_1_test['First_Derivative'].shift(1)\n",
    "    df_1_test.loc[:,'Peak'] = df_1_test.apply(lambda x: 1 if (x['Lag_First_Derivative'] > 0 and x['First_Derivative'] < 0) else 0, axis = 1)\n",
    "#   df_1_test.loc[:,'PC_Ratio_EWM20'] = df_1_test['Put_Call_Ratio'].ewm(span = 20).mean()\n",
    "    # 20 day's EWM for PC_Ratio b/c https://www.investopedia.com/trading/forecasting-market-direction-with-put-call-ratios/\n",
    "    \n",
    "    indicators = pd.DataFrame({'Index':[0],\n",
    "                               'RSI':[0],\n",
    "                              'Std.dev_PE':[0],\n",
    "                              'Avg_PE':[0],\n",
    "                              'PE':[0],\n",
    "                              'Recent_Peak':[0],\n",
    "                              'MoM':[0],\n",
    "                              '3M_MoM':[0],\n",
    "                              'YoY':[0],\n",
    "                              'Max_YoY':[0],\n",
    "                              '50_MA':[0],\n",
    "                              '200_MA':[0],\n",
    "                              '50_day_test_peak':[0],\n",
    "                              '50_day_test_trough':[0],\n",
    "                              'Min_YoY':[0],\n",
    "                              'PC_EWM20':[0]})\n",
    "    \n",
    "    # one year look back period for PE\n",
    "    end_date = '2019-12-31' # we can adjust this to the last date of the data, use this for backtest \n",
    "    end = dt.datetime.strptime(end_date, '%Y-%m-%d') # we can adjust this to the last date of the data, use this for backtest \n",
    "    start = end - dt.timedelta(days = 365)\n",
    "    indicators['Index'] = df_1_test.loc[end,'Index']\n",
    "    indicators['RSI'] = df_1_test.loc[end,'RSI']\n",
    "    indicators['Std.dev_PE'] = df_1_test.loc[start:end,'PE_Ratio'].std()\n",
    "    indicators['Avg_PE'] = df_1_test.loc[start:end,'PE_Ratio'].mean()\n",
    "    indicators['PE'] = df_1_test.loc[end,'PE_Ratio']\n",
    "    indicators['Recent_Peak'] = df_1_test[df_1_test.Peak==1].iloc[-1]['Index']\n",
    "    indicators['MoM'] = df_1_test.loc[end,'MoM_change']\n",
    "    indicators['3M_MoM'] = df_1_test.loc['2019-10-01':end,'MoM_change'].mean()\n",
    "    indicators['YoY'] = df_1_test.loc[end,'YoY_change']\n",
    "    indicators['Max_YoY'] = df_1_test['YoY_change'].rolling(window = 252).max()[-1]\n",
    "    indicators['Min_YoY'] = df_1_test['YoY_change'].rolling(window = 252).min()[-1]\n",
    "    indicators['50_MA'] = df_1_test.iloc[-1]['50_MA']\n",
    "    indicators['200_MA'] = df_1_test.iloc[-1]['200_MA']\n",
    "    # 1 day shift for the time being, may change to 10 depending on what Laurence prefers\n",
    "    indicators['50_day_test_peak'] = ((df_1_test['50_MA']- df_1_test['200_MA']).shift(periods = 1).rolling(window = 126).min() > 0) [-1]\n",
    "    indicators['50_day_test_trough'] = ((df_1_test['50_MA']- df_1_test['200_MA']).shift(periods = 1).rolling(window = 126).max() < 0) [-1]\n",
    "    # indicators['PC_EWM20'] = df_1_test.iloc[-1]['PC_Ratio_EWM20']\n",
    "    \n",
    "    \n",
    "    check_peak_overbought = ((indicators['RSI'] >70) & (indicators['PE'] > (indicators['Avg_PE']+indicators['Std.dev_PE'])))\n",
    "    check_peak_candy_fish = ((indicators['Index'] < indicators['Recent_Peak']*0.98) & ((indicators['MoM']> 0) & (indicators['3M_MoM'] > 0)))\n",
    "    check_peak_rolled_over = ((indicators['YoY'] >=  indicators['Max_YoY']*0.5))\n",
    "    check_peak_crossed_over = ((indicators['50_MA']< indicators['200_MA']) & indicators['50_day_test_peak'])\n",
    "    # check_peak_put_call = indicators['PC_EWM20'] >= 2\n",
    "    \n",
    "    check_trough_overbought = ((indicators['RSI'] <30) & (indicators['PE'] < (indicators['Avg_PE']-indicators['Std.dev_PE']))) \n",
    "    check_trough_candy_fish = ((indicators['Index'] > indicators['Recent_Peak']*1.02) & ((indicators['MoM'] < 0) & (indicators['3M_MoM'] <0))) \n",
    "    check_trough_rolled_over = ((indicators['YoY'] <=  indicators['Min_YoY']*0.5)) \n",
    "    check_trough_crossed_over = ((indicators['50_MA'] > indicators['200_MA']) & (indicators['50_day_test_trough']))    \n",
    "    # check_trough_put_call = indicators['PC_EWM20'] <= 1/2\n",
    "    \n",
    "    all_indicators = all_indicators.append(indicators, ignore_index = True)\n",
    "    \n",
    "    # trend analysis\n",
    "    def trend_analysis(dataset):\n",
    "        trend_data = LinearTrend(dataset)   \n",
    "        trend_indicators = pd.DataFrame({'Return_to_trend':[0], # Sanchita\n",
    "                                          'Trend_adj_peak_trough':[0], # Yushi\n",
    "                                          'SD_moves':[0], # Jo\n",
    "                                          'Num_above_trend':[0], # Shubham\n",
    "                                          'Num_below_trend':[0],\n",
    "                                          'Num_above_200MA':[0],\n",
    "                                          'Num_below_200MA':[0]}) # Shubham\n",
    "                \n",
    "        trend_indicators['Trend_adj_peak_trough'] = TrendAdjPT(dataset, trend_data)\n",
    "        trend_indicators[['Num_above_trend', 'Num_below_trend', 'Num_above_200MA', 'Num_below_200MA']] = Days_above_below(dataset, trend_data)\n",
    "        trend_indicators['SD_moves'] = SDmoves(ind_data = dataset, trend_data = trend_data)\n",
    "        trend_indicators['Return_to_trend'] = Perc_above_below(dataset, trend_data)\n",
    "        return trend_indicators\n",
    "    \n",
    "    if (check_peak_overbought[0]== True):\n",
    "        #candidate_pairs_overbought = candidate_pairs_overbought.append({'peaking':all_combinations[i]}, ignore_index = True)\n",
    "        temp_trend = trend_analysis(df_1_test['Index'])\n",
    "        temp_trend['peaking'] = all_combinations[i]\n",
    "        temp_trend['troughing'] = np.NaN\n",
    "        candidate_pairs_overbought = candidate_pairs_overbought.append(temp_trend, ignore_index = True)\n",
    "    elif(check_trough_overbought[0]==True):\n",
    "        #candidate_pairs_overbought = candidate_pairs_overbought.append({'troughing':all_combinations[i]}, ignore_index = True)\n",
    "        temp_trend = trend_analysis(df_1_test['Index'])\n",
    "        temp_trend['peaking'] = np.NaN\n",
    "        temp_trend['troughing'] = all_combinations[i] \n",
    "        candidate_pairs_overbought = candidate_pairs_overbought.append(temp_trend, ignore_index = True)\n",
    "    \n",
    "    if (check_peak_candy_fish[0]== True):\n",
    "        #candidate_pairs_candy_fish = candidate_pairs_candy_fish.append({'peaking':all_combinations[i]}, ignore_index = True)\n",
    "        temp_trend = trend_analysis(df_1_test['Index'])\n",
    "        temp_trend['peaking'] = all_combinations[i]\n",
    "        temp_trend['troughing'] = np.NaN\n",
    "        candidate_pairs_candy_fish = candidate_pairs_candy_fish.append(temp_trend, ignore_index = True)\n",
    "    elif(check_trough_candy_fish[0]==True):\n",
    "        #candidate_pairs_candy_fish = candidate_pairs_candy_fish.append({'troughing':all_combinations[i]}, ignore_index = True)\n",
    "        temp_trend = trend_analysis(df_1_test['Index'])\n",
    "        temp_trend['peaking'] = np.NaN\n",
    "        temp_trend['troughing'] = all_combinations[i]\n",
    "        candidate_pairs_candy_fish = candidate_pairs_candy_fish.append(temp_trend, ignore_index = True)\n",
    "        \n",
    "    if (check_peak_rolled_over[0]== True):\n",
    "        #candidate_pairs_rolled_over = candidate_pairs_rolled_over.append({'peaking':all_combinations[i]}, ignore_index = True)\n",
    "        temp_trend = trend_analysis(df_1_test['Index'])\n",
    "        temp_trend['peaking'] = all_combinations[i]\n",
    "        temp_trend['troughing'] = np.NaN\n",
    "        candidate_pairs_rolled_over = candidate_pairs_rolled_over.append(temp_trend, ignore_index = True)\n",
    "    elif(check_trough_rolled_over[0]==True):\n",
    "        #candidate_pairs_rolled_over = candidate_pairs_rolled_over.append({'troughing':all_combinations[i]}, ignore_index = True)\n",
    "        temp_trend = trend_analysis(df_1_test['Index'])\n",
    "        temp_trend['peaking'] =  np.NaN\n",
    "        temp_trend['troughing'] = all_combinations[i]\n",
    "        candidate_pairs_rolled_over = candidate_pairs_rolled_over.append(temp_trend, ignore_index = True)\n",
    "        \n",
    "    if (check_peak_crossed_over[0]== True):\n",
    "        #candidate_pairs_crossed_over = candidate_pairs_crossed_over.append({'peaking':all_combinations[i]}, ignore_index = True)\n",
    "        temp_trend = trend_analysis(df_1_test['Index'])\n",
    "        temp_trend['peaking'] = all_combinations[i] \n",
    "        temp_trend['troughing'] = np.NaN\n",
    "        candidate_pairs_crossed_over = candidate_pairs_crossed_over.append(temp_trend, ignore_index = True)\n",
    "    elif(check_trough_crossed_over[0]==True):\n",
    "        #candidate_pairs_crossed_over = candidate_pairs_crossed_over.append({'troughing':all_combinations[i]}, ignore_index = True)\n",
    "        temp_trend = trend_analysis(df_1_test['Index'])\n",
    "        temp_trend['peaking'] =  np.NaN\n",
    "        temp_trend['troughing'] = all_combinations[i]\n",
    "        candidate_pairs_crossed_over = candidate_pairs_crossed_over.append(temp_trend, ignore_index = True)\n",
    "    \n",
    "# =============================================================================\n",
    "#     # not running p/c for now since there are a lot of pairs here\n",
    "#     if (check_peak_put_call[0]== True):\n",
    "#         temp_trend = trend_analysis(df_1['Index'])\n",
    "#         temp_trend['peaking'] = all_combinations[i] \n",
    "#         temp_trend['troughing'] = np.NaN\n",
    "#         candidate_pairs_put_call = candidate_pairs_put_call.append(temp_trend, ignore_index = True)\n",
    "#     elif(check_trough_put_call[0]==True):\n",
    "#         # candidate_pairs_put_call = candidate_pairs_put_call.append({'troughing':all_combinations[i]}, ignore_index = True)\n",
    "#         temp_trend = trend_analysis(df_1['Index'])\n",
    "#         temp_trend['peaking'] = np.NaN \n",
    "#         temp_trend['troughing'] = all_combinations[i]\n",
    "#         candidate_pairs_put_call = candidate_pairs_put_call.append(temp_trend, ignore_index = True)\n",
    "# =============================================================================\n",
    "        \n",
    "    \n",
    "    clear_output(wait = True)\n",
    "\n",
    "end = dt.datetime.now()\n",
    "end - start\n",
    "\n",
    "\n",
    "# In[6]:\n",
    "\n",
    "\n",
    "candidate_pairs_overbought.to_excel(\"candidate_pairs_overbought.xlsx\")\n",
    "candidate_pairs_candy_fish.to_excel(\"candidate_pairs_candy_fish.xlsx\")\n",
    "candidate_pairs_rolled_over.to_excel(\"candidate_pairs_rolled_over.xlsx\")\n",
    "candidate_pairs_crossed_over.to_excel(\"candidate_pairs_crossed_over.xlsx\")\n",
    "# candidate_pairs_put_call.to_excel('candidate_pairs_put_call.xlsx')\n",
    "\n",
    "all_indicators = all_indicators.loc[1:len(all_indicators)]\n",
    "all_indicators.index = all_combinations\n",
    "\n",
    "all_indicators.to_excel('all_indicators.xlsx')\n",
    "\n",
    "\n",
    "# # Loading Selected Candidated Pairs\n",
    "\n",
    "# In[4]:\n",
    "\n",
    "\n",
    "candidate_pairs_overbought = pd.read_excel(\"candidate_pairs_overbought.xlsx\")\n",
    "candidate_pairs_candy_fish = pd.read_excel(\"candidate_pairs_candy_fish.xlsx\")\n",
    "candidate_pairs_rolled_over = pd.read_excel(\"candidate_pairs_rolled_over.xlsx\")\n",
    "candidate_pairs_crossed_over = pd.read_excel(\"candidate_pairs_crossed_over.xlsx\")\n",
    "\n",
    "\n",
    "# ## Ranking Pairs and Selecting Top 10 in each\n",
    "\n",
    "# In[26]:\n",
    "\n",
    "\n",
    "for i in [candidate_pairs_overbought, candidate_pairs_candy_fish, candidate_pairs_rolled_over, candidate_pairs_crossed_over]:\n",
    "    i['Rank_Return_to_trend'] = abs(i['Return_to_trend'].rank())\n",
    "    i['Rank_Trend_adj_peak_trough'] = abs(i['Trend_adj_peak_trough'].rank())\n",
    "    i['Rank_SD_moves'] = i['SD_moves'].rank()\n",
    "    i['Rank_Num_trend'] = np.where(i['peaking'].isna(), i['Num_below_trend'], i['Num_above_trend'])\n",
    "    i['Rank_Num_trend'] = i['Rank_Num_trend'].rank()\n",
    "    i['Rank_Num_200MA'] = np.where(i['peaking'].isna(), i['Num_below_200MA'], i['Num_above_200MA'])\n",
    "    i['Rank_Num_200MA'] = i['Rank_Num_200MA'].rank()\n",
    "    i['avg_rank'] = (i['Rank_Return_to_trend'] + i['Rank_Trend_adj_peak_trough'] + i['Rank_SD_moves'] + i['Rank_Num_trend'] + i['Rank_Num_200MA'])/5\n",
    "    i['Pair'] = np.where(i['peaking'].isna(), i['troughing'], i['peaking'])\n",
    "\n",
    "\n",
    "# In[78]:\n",
    "\n",
    "\n",
    "top10 = np.array([])\n",
    "for i in [candidate_pairs_overbought, candidate_pairs_candy_fish, candidate_pairs_rolled_over, candidate_pairs_crossed_over]:\n",
    "    top10 = np.append(top10, i.sort_values(by = ['avg_rank'])['Pair'][0:10].values)\n",
    "\n",
    "\n",
    "# In[4]:\n",
    "\n",
    "\n",
    "all_data_2 = pd.read_excel(\"Final_Data.xlsx\", sheet_name = \"Sheet3\", header = [0,1])\n",
    "# all_data_2 = pd.read_excel(\"Data_Full_Editable.xlsx\", sheet_name = \"Sheet3\", header = [0,1])\n",
    "\n",
    "all_data_2.index = all_data_2['Unnamed: 0_level_0']['Dates']\n",
    "\n",
    "all_data_2_raw = all_data_2.drop(columns ='Unnamed: 0_level_0' )\n",
    "\n",
    "all_data_2 = all_data_2_raw[all_data_2_raw.index <= \"2019-12-31\"] # specify end_date\n",
    "\n",
    "price_data = all_data_2.xs('PX_LAST', axis = 1, level = 1, drop_level = False) # subsetting to PX_LAST only\n",
    "\n",
    "\n",
    "# # Clean Factors\n",
    "\n",
    "# In[5]:\n",
    "\n",
    "# read in macro and feedstock factors\n",
    "macro_factors_raw = pd.read_excel('BBGFactors.xlsx', sheet_name = 'macroFactors')\n",
    "feedstock_factors_raw = pd.read_excel('BBGFactors.xlsx', sheet_name = 'feedstockFactors')\n",
    "\n",
    "def CleanFactors(df, cut_off_date):\n",
    "    # preps the df for further analysis, specify date in this format '2020-08-20'\n",
    "    df.index = df.Dates\n",
    "    date_loc = np.where(df.index == cut_off_date)[0]\n",
    "    if len(date_loc) == 0: return 'Invalid date'\n",
    "    date_loc = date_loc[0]\n",
    "    df = df.loc[df.index[:date_loc]]\n",
    "    df = df.drop('Dates', axis = 1)\n",
    "    df = df.ffill()\n",
    "    df = df.dropna(axis = 1)\n",
    "    return df\n",
    "\n",
    "cut_off_date = '2019-12-31'\n",
    "macro_factors_round1 = CleanFactors(macro_factors_raw, cut_off_date)\n",
    "feedstock_factors_round1 = CleanFactors(feedstock_factors_raw, cut_off_date)\n",
    "\n",
    "# YoY adjustment\n",
    "def YoYClean(factors_df):\n",
    "    macro_names = factors_df.columns.tolist()\n",
    "    pct_yoy = ['YoY' in x for x in macro_names]\n",
    "    factors_df_adj = pd.DataFrame(np.NaN, index = factors_df.index, columns = macro_names)\n",
    "    for i in range(len(macro_names)):\n",
    "        if pct_yoy[i] == True:\n",
    "            factors_df_adj.iloc[:,i] = (factors_df.iloc[:,i] / factors_df.iloc[:,i].shift(252) - 1) * 100\n",
    "        else:\n",
    "            factors_df_adj.iloc[:,i] = factors_df.iloc[:,i]\n",
    "    \n",
    "    factors_df_adj = factors_df_adj.dropna(axis = 0) # drop observations instead of columns this time\n",
    "    return factors_df_adj\n",
    "\n",
    "macro_factors = YoYClean(macro_factors_round1) / 100\n",
    "feedstock_factors = YoYClean(feedstock_factors_round1) \n",
    "feedstock_factors = (feedstock_factors_round1 / feedstock_factors_round1.shift(252) - 1) * 100\n",
    "feedstock_factors = feedstock_factors.dropna(axis = 0) / 100\n",
    "\n",
    "\n",
    "\n",
    "# # Stepwise Regression\n",
    "\n",
    "# In[381]:\n",
    "\n",
    "\n",
    "top10_pairs = ['AXTA US Equity_HXL US Equity','AVD US Equity_PRLB US Equity','AVD US Equity_TSE US Equity','AVD US Equity_DOW US Equity',\n",
    "               'AVD US Equity_GCP US Equity','AVD US Equity_VVV US Equity','AVD US Equity_IMCD NA Equity','ADM US Equity_HXL US Equity',\n",
    "               'AVD US Equity_WDFC US Equity','AVD US Equity_WLK US Equity','CINR US Equity_CE US Equity','GCP US Equity_OEC US Equity',\n",
    "               'RPM US Equity_UNVR US Equity','ALB US Equity_CC US Equity','CRDA LN Equity_IMCD NA Equity','DSM NA Equity_OEC US Equity',\n",
    "               'NZYMB DC Equity_OEC US Equity','CINR US Equity_OLN US Equity','KOP US Equity_MMM US Equity','MMM US Equity_OEC US Equity',\n",
    "               'AXTA US Equity_KWR US Equity','GPRE US Equity_KWR US Equity','AXTA US Equity_DD US Equity','AXTA US Equity_MMM US Equity',\n",
    "               'AXTA US Equity_HXL US Equity','GPRE US Equity_TSE US Equity','GPRE US Equity_PRLB US Equity','DCI US Equity_KWR US Equity',\n",
    "               'KOP US Equity_TSE US Equity','AVD US Equity_TSE US Equity','AVD US Equity_BAS GR Equity','AXTA US Equity_NZYMB DC Equity',\n",
    "               'AVD US Equity_CBT US Equity','GPRE US Equity_RPM US Equity','AVD US Equity_ADM US Equity']\n",
    "\n",
    "\n",
    "# In[382]:\n",
    "\n",
    "\n",
    "price_data = all_data_2.xs('PX_LAST', axis = 1, level = 1, drop_level = False) # subsetting to PX_LAST only\n",
    "price_data.columns = price_data.columns.droplevel(level = 1)\n",
    "#price_data = price_data.loc['2015-01-01':]\n",
    "#price_data = price_data.dropna(axis = 1)\n",
    "\n",
    "\n",
    "# In[383]:\n",
    "\n",
    "\n",
    "stepwise_factors = pd.DataFrame({'pair':top10_pairs,\n",
    "             'factor_1':[np.nan]*len(top10_pairs),\n",
    "             'lag_1':[np.nan]*len(top10_pairs),\n",
    "             'factor_2':[np.nan]*len(top10_pairs),\n",
    "             'lag_2':[np.nan]*len(top10_pairs),\n",
    "             'factor_3':[np.nan]*len(top10_pairs),\n",
    "             'lag_3':[np.nan]*len(top10_pairs),\n",
    "             'factor_4':[np.nan]*len(top10_pairs),\n",
    "             'lag_4':[np.nan]*len(top10_pairs),\n",
    "             'factor_5':[np.nan]*len(top10_pairs),\n",
    "             'lag_5':[np.nan]*len(top10_pairs)})\n",
    "\n",
    "\n",
    "# In[384]:\n",
    "\n",
    "\n",
    "def R_2_stepwise(main_data, factor_data, lag):\n",
    "    main_data_w_factor = pd.merge(main_data, factor, left_index = True, right_index = True, how = 'left')\n",
    "    main_data_w_factor['lag_factor'] = main_data_w_factor.iloc[:,1].shift(22*lag)\n",
    "    main_data_w_factor = main_data_w_factor.dropna()\n",
    "    if (main_data_w_factor.shape[0]>0):\n",
    "        lm = LinearRegression()\n",
    "        lm.fit(main_data_w_factor[['lag_factor']], main_data_w_factor[['Index']],)\n",
    "        R_2 = np.round(lm.score(main_data_w_factor[['lag_factor']], main_data_w_factor[['Index']]),5)\n",
    "    else:\n",
    "        R_2 = 0\n",
    "    return(R_2)\n",
    "\n",
    "\n",
    "# In[385]:\n",
    "\n",
    "\n",
    "all_factors = pd.merge(macro_factors, feedstock_factors, left_on = macro_factors.index, right_on = feedstock_factors.index, how = 'outer')\n",
    "all_factors.index = all_factors.key_0\n",
    "all_factors.index.name = 'Date'\n",
    "all_factors = all_factors.drop(columns = ['key_0'])\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "for pair in range(len(top10_pairs)):\n",
    "    print(pair)\n",
    "        \n",
    "    ################ Get all Highest R_2 #######################\n",
    "    \n",
    "    current_factor_level = 1\n",
    "    \n",
    "    R_2_df = pd.DataFrame({'factor_name':[],\n",
    "                       'lag':[],\n",
    "                      'R_2':[]})\n",
    "    \n",
    "    highest_R_2_factor = np.array([])\n",
    "    lag_of_highest_R_2 = np.array([])\n",
    "    \n",
    "    # insert code to read the candidate pairs list\n",
    "    pair_names = top10_pairs[pair]\n",
    "    pair_names = pair_names.split('_')\n",
    "    \n",
    "    main_data = price_data.loc[:,price_data.columns.isin(pair_names)].copy().dropna()\n",
    "    \n",
    "    if main_data.shape[1] < 2:\n",
    "        continue\n",
    "    \n",
    "    main_data['Index'] = (main_data.iloc[:,0]/main_data.iloc[:,1])\n",
    "    main_data[['Index']] = (main_data[['Index']].shift(-21*6) - main_data[['Index']])/main_data[['Index']]\n",
    "    factors_to_consider = all_factors.columns\n",
    "    \n",
    "    for j in factors_to_consider:\n",
    "        for lag in range(1,19,1):\n",
    "            factor = all_factors.loc[:,[j]]\n",
    "            R_2 = R_2_stepwise(main_data[['Index']], factor, lag)\n",
    "            R_2_df = R_2_df.append({'factor_name':j,\n",
    "                          'lag':lag,\n",
    "                          'R_2':R_2}, ignore_index = True)\n",
    "    \n",
    "    highest_R_2_factor = np.append(highest_R_2_factor, np.array(R_2_df.loc[R_2_df.R_2 == max(R_2_df.R_2),'factor_name']))\n",
    "    lag_of_highest_R_2 = np.append(lag_of_highest_R_2, int(np.array(R_2_df.loc[R_2_df.R_2 == max(R_2_df.R_2),'lag'])[0]))\n",
    "    \n",
    "    stepwise_factors.loc[stepwise_factors.pair == top10_pairs[pair],'factor_1'] = highest_R_2_factor[0]\n",
    "    stepwise_factors.loc[stepwise_factors.pair == top10_pairs[pair],'lag_1'] = lag_of_highest_R_2[0]\n",
    "    \n",
    "    \n",
    "    ####################### Step 2 in Stepwise Regression #####################\n",
    "    \n",
    "    for i in range(4):\n",
    "        \n",
    "        current_factor_level = i+2\n",
    "\n",
    "        factors_to_consider = factors_to_consider[~factors_to_consider.isin(highest_R_2_factor)]    \n",
    "        ## We need the error with highest R_2 factor then run with other remaining factors\n",
    "\n",
    "        ##### Extract errors from first factor regression #####\n",
    "\n",
    "        reg_data = pd.merge(main_data[['Index']], all_factors.loc[:,highest_R_2_factor[0:(i+1)]], left_index = True, right_index = True, how = 'left')\n",
    "        for main_lag in range(i+1):\n",
    "            reg_data[['lag_factor_' + str(main_lag)]] = reg_data.loc[:,[highest_R_2_factor[main_lag]]].shift(22*int(lag_of_highest_R_2[main_lag-1]))\n",
    "        reg_data = reg_data.dropna()\n",
    "        lm = LinearRegression()\n",
    "        lm.fit(reg_data[['lag_factor_' + str(k) for k in range(i+1)]], reg_data[['Index']])\n",
    "        errors = lm.predict(reg_data[['lag_factor_' + str(k) for k in range(i+1)]]) - reg_data[['Index']]\n",
    "        errors.columns = ['Index']\n",
    "\n",
    "        R_2_df = pd.DataFrame({'factor_name':[],\n",
    "                           'lag':[],\n",
    "                          'R_2':[]})\n",
    "\n",
    "        for j in factors_to_consider:\n",
    "            for lag in range(1,19,1):\n",
    "                factor = all_factors.loc[:,[j]]\n",
    "                R_2 = R_2_stepwise(errors, factor, lag)\n",
    "                R_2_df = R_2_df.append({'factor_name':j,\n",
    "                              'lag':lag,\n",
    "                              'R_2':R_2}, ignore_index = True)\n",
    "\n",
    "        highest_R_2_factor = np.append(highest_R_2_factor,np.array(R_2_df.loc[R_2_df.R_2 == max(R_2_df.R_2),'factor_name']))\n",
    "        lag_of_highest_R_2 = np.append(lag_of_highest_R_2, int(np.array(R_2_df.loc[R_2_df.R_2 == max(R_2_df.R_2),'lag'])[0]))\n",
    "\n",
    "        stepwise_factors.loc[stepwise_factors.pair == top10_pairs[pair],f'factor_{i+2}'] = highest_R_2_factor[i+1]\n",
    "        stepwise_factors.loc[stepwise_factors.pair == top10_pairs[pair],f'lag_{i+2}'] = lag_of_highest_R_2[i+1]      \n",
    "\n",
    "        factors_to_consider = factors_to_consider[~factors_to_consider.isin(highest_R_2_factor)]   \n",
    "            \n",
    "            \n",
    "\n",
    "\n",
    "# In[391]:\n",
    "\n",
    "\n",
    "stepwise_factors.to_csv(\"stepwise_factors.csv\")\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "# lag factors implementing\n",
    "lag_factors = pd.read_csv(r'C:\\Users\\yushi\\Documents\\GitHub\\AFP2020\\stepwise_factors.csv')\n",
    "lag_factors.columns\n",
    "\n",
    "def CheckDate(date_in,this_list):\n",
    "    while date_in not in this_list:\n",
    "        date_in -= dt.timedelta(days = 1)\n",
    "    return date_in\n",
    "\n",
    "\n",
    "prediction_direction = dict.fromkeys(top10_pairs)\n",
    "\n",
    "# reading in raw data, adapted from data_cleaning\n",
    "all_data_2 = pd.read_excel(\"data_0719.xlsx\", sheet_name = \"Sheet5\", header = [0,1])\n",
    "all_data_2.index = all_data_2['Unnamed: 0_level_0']['Dates']\n",
    "all_data_2_raw = all_data_2.drop(columns ='Unnamed: 0_level_0' )\n",
    "price_data = all_data_2.xs('PX_LAST', axis = 1, level = 1, drop_level = False) # subsetting to PX_LAST only\n",
    "\n",
    "def PlotIndex(i):\n",
    "    pair_names = top10_pairs[i]\n",
    "    pair_names = pair_names.split('_')\n",
    "    \n",
    "    # calculating the index as the ratio of PX_LAST\n",
    "    index_level = price_data[pair_names[0]] / price_data[pair_names[1]]\n",
    "    index_level = index_level.dropna()\n",
    "    \n",
    "    index_level.plot(title = top10_pairs[i])\n",
    "\n",
    "for i in [x for x in range(len(top10_pairs)) if x != 3]:\n",
    "    PlotIndex(-1)\n",
    "\n",
    "# could also add a portion for back test...i.e. calculate the actual results vs. predicted\n",
    "    \n",
    "\n",
    "for i in [x for x in range(len(top10_pairs)) if x != 3]:\n",
    "    # insert code to read the candidate pairs list\n",
    "    pair_names = top10_pairs[i]\n",
    "    pair_names = pair_names.split('_')\n",
    "    \n",
    "    # reading in raw data, adapted from data_cleaning\n",
    "    all_data_2 = pd.read_excel(\"data_0719.xlsx\", sheet_name = \"Sheet5\", header = [0,1])\n",
    "    all_data_2.index = all_data_2['Unnamed: 0_level_0']['Dates']\n",
    "    all_data_2_raw = all_data_2.drop(columns ='Unnamed: 0_level_0' )\n",
    "    all_data_2 = all_data_2_raw[all_data_2_raw.index <= \"2019-12-31\"] # specify end_date\n",
    "    price_data = all_data_2.xs('PX_LAST', axis = 1, level = 1, drop_level = False) # subsetting to PX_LAST only\n",
    "    \n",
    "    # calculating the index as the ratio of PX_LAST\n",
    "    index_level = price_data[pair_names[0]] / price_data[pair_names[1]]\n",
    "    index_level = index_level.loc[macro_factors.index] # aligns the y data with the x data\n",
    "    index_level = index_level.dropna()\n",
    "    \n",
    "    # temp_macro = factors_cleaning.macro_factors.loc[index_level.index]\n",
    "    # temp_feedstock = factors_cleaning.feedstock_factors.loc[index_level.index]\n",
    "    temp_macro = macro_factors.loc[index_level.index]\n",
    "    temp_feedstock = feedstock_factors.loc[index_level.index]\n",
    "    \n",
    "    # we will drop all indicators that do not have 10 years of data completely\n",
    "    # forward fill on the remaining to close the NAs gap\n",
    "    \n",
    "    # replacing na's with 0s\n",
    "    temp_macro = temp_macro.fillna(0)\n",
    "    temp_feedstock = temp_feedstock.fillna(0)\n",
    "\n",
    "    # need a day input, [last day of the training dataset]\n",
    "    # puting in an arbitrary date for now\n",
    "    input_date = dt.date(2019,12,30)\n",
    "    \n",
    "    nameP = top10_pairs[i]\n",
    "    row_num = np.where(nameP == lag_factors.pair)[0][0]\n",
    "    lag_factor_names = lag_factors.loc[row_num,['factor_1','factor_2','factor_3','factor_4','factor_5']]\n",
    "    lag_factor_times = lag_factors.loc[row_num,['lag_1','lag_2','lag_3','lag_4','lag_5']]\n",
    "    days_lag = lag_factor_times * 21 # converting to days equivalent\n",
    "    spec_dates = [input_date - dt.timedelta(days = int(x)) for x in days_lag.tolist()]\n",
    "    lag_dates = spec_dates\n",
    "    \n",
    "    \n",
    "    \n",
    "    X_vars = pd.DataFrame(np.NaN, index = index_level.index, columns = lag_factor_names.tolist())\n",
    "    \n",
    "    # reading in the time series and lagging it by that much\n",
    "    for z in range(len(lag_factor_names)):\n",
    "        if lag_factor_names[z] in temp_macro.columns:\n",
    "            temp = temp_macro.loc[:,lag_factor_names[z]].copy()\n",
    "            temp = temp.shift(int(days_lag[z])) # takes care of the lag\n",
    "            X_vars.loc[:,lag_factor_names[z]] = temp\n",
    "        else:\n",
    "            temp = temp_feedstock.loc[:,lag_factor_names[z]].copy()\n",
    "            temp = temp.shift(int(days_lag[z])) # takes care of the lag\n",
    "            X_vars.loc[:,lag_factor_names[z]] = temp\n",
    "    \n",
    "    X_vars = X_vars.dropna(axis = 0)\n",
    "    \n",
    "    Y_var = index_level.shift(periods = -21*6) - index_level # 6 months forward\n",
    "    \n",
    "    # matching the dates\n",
    "    joint_df = X_vars.merge(Y_var, how = 'outer', left_index = True, right_index = True)\n",
    "    last_day_X = joint_df.iloc[-1,:].drop('PX_LAST', axis = 0)\n",
    "    joint_df = joint_df.dropna()\n",
    "    features = joint_df.copy()\n",
    "    \n",
    "    ################################ RF Implementation ################################\n",
    "    \n",
    "    # labels are the values we want to predict\n",
    "    labels = np.array(features['PX_LAST'])\n",
    "    \n",
    "    # Remove the labels from the features\n",
    "    features = features.drop('PX_LAST', axis = 1)\n",
    "    \n",
    "    # Saving feature names for later use\n",
    "    feature_list = list(features.columns)\n",
    "    \n",
    "    # Convert to numpy array\n",
    "    features = np.array(features)\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    rf = RandomForestRegressor(random_state = 42)\n",
    "    from sklearn.model_selection import RandomizedSearchCV\n",
    "    \n",
    "    # Number of trees in random forest\n",
    "    n_estimators = [int(x) for x in np.linspace(start = 25, stop = 150, num = 15)]\n",
    "    # Number of features to consider at every split\n",
    "    max_features = ['auto', 'sqrt']\n",
    "    # Maximum number of levels in tree\n",
    "    max_depth = [int(x) for x in np.linspace(3, 15, num = 2)]\n",
    "    # Minimum number of samples required to split a node\n",
    "    min_samples_split = [50,60]\n",
    "    # Minimum number of samples required at each leaf node\n",
    "    min_samples_leaf = [5,10,15]\n",
    "    # Method of selecting samples for training each tree\n",
    "    bootstrap = [True]\n",
    "    # Create the random grid\n",
    "    random_grid = {'n_estimators':n_estimators,\n",
    "                   'max_features':max_features,\n",
    "                   'max_depth':max_depth,\n",
    "                   'min_samples_split':min_samples_split,\n",
    "                   'min_samples_leaf':min_samples_leaf,\n",
    "                   'bootstrap':bootstrap}\n",
    "    start_time = dt.datetime.now()\n",
    "    t_features = features\n",
    "    t_labels = labels\n",
    "    # Use the random grid to search for best hyperparameters\n",
    "    # First create the base model to tune\n",
    "    rf = RandomForestRegressor()\n",
    "    # Random search of parameters, using 3 fold cross validation, \n",
    "    # search across 100 different combinations, and use all available cores\n",
    "    rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, \n",
    "                                   n_iter = 100, cv = 3, verbose=0, \n",
    "                                   n_jobs = -1)\n",
    "    # Fit the random search model\n",
    "    rf_random.fit(t_features, t_labels)\n",
    "    \n",
    "    rf_random.best_params_\n",
    "    \n",
    "    # the random search picks different combinations, whereas the grid search is more refinement but slower\n",
    "    end = dt.datetime.now()\n",
    "    print(end - start_time)\n",
    "    \n",
    "    # Using Skicit-learn to split data into training and testing sets\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    # Split the data into training and testing sets\n",
    "    train_features, test_features, train_labels, test_labels = train_test_split(features, labels, test_size = 0.20, random_state = 42)\n",
    "    \n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "    # Create the parameter grid based on the results of random search    \n",
    "    def GridAdjust(rf_best_params):\n",
    "        temp_grid = rf_best_params.copy()\n",
    "        p_grid = temp_grid.copy()\n",
    "        p_grid['max_depth'] = [temp_grid['max_depth']-1, temp_grid['max_depth']+1]\n",
    "        p_grid['min_samples_leaf'] = [max(temp_grid['min_samples_leaf']-5,1), temp_grid['min_samples_leaf']+5]\n",
    "        p_grid['min_samples_split'] = [max(temp_grid['min_samples_split']-10,1), temp_grid['min_samples_split']+10]\n",
    "        p_grid['n_estimators'] = [max(temp_grid['n_estimators']-15,1), temp_grid['n_estimators']+15]\n",
    "        p_grid['max_features'] = [temp_grid['max_features']]\n",
    "        p_grid['bootstrap'] = [temp_grid['bootstrap']]\n",
    "        return p_grid\n",
    "    \n",
    "    param_grid = GridAdjust(rf_random.best_params_)\n",
    "    \n",
    "    # Create a base model\n",
    "    rf = RandomForestRegressor()\n",
    "    # Instantiate the grid search model\n",
    "    grid_search = GridSearchCV(estimator = rf, param_grid = param_grid, \n",
    "                              cv = 3, n_jobs = -1, verbose = 0)\n",
    "    \n",
    "    grid_search.fit(t_features, t_labels)\n",
    "    grid_search.best_params_\n",
    "    \n",
    "    best_grid = grid_search.best_estimator_\n",
    "    \n",
    "    def evaluate(model, test_features, test_labels):\n",
    "        predictions = model.predict(test_features)\n",
    "        errors = abs(predictions - test_labels)\n",
    "        mape = 100 * np.mean(errors / test_labels)\n",
    "        RMSE = np.sqrt(np.mean(np.square(abs(predictions - test_labels))))\n",
    "        print('Model Performance')\n",
    "        print('Average Error: {:0.4f}'.format(np.mean(errors)))\n",
    "        print('RMS Error: {:0.4f}'.format(RMSE))\n",
    "        return mape\n",
    "    \n",
    "    grid_test_mape = evaluate(best_grid, test_features, test_labels)\n",
    "    grid_train_mape = evaluate(best_grid, train_features, train_labels)\n",
    "    \n",
    "    predictions_final = best_grid.predict(np.array(last_day_X).reshape(1,5))\n",
    "    prediction_direction[top10_pairs[i]] = np.sign(predictions_final[0])\n",
    "    # +'ve, therefore peaking\n",
    "    \n",
    "    ## logistics is % change instead.... (6 month ahead - today)/today's level\n",
    "    ## check to see if it is greater than 15% threshold \n",
    "\n",
    "# pair 3 is blank...filling it in for now\n",
    "    \n",
    "# saving pickle \n",
    "filename = 'prediction_direction'\n",
    "outfile = open(filename, 'wb')\n",
    "pickle.dump(prediction_direction, outfile)\n",
    "outfile.close()\n",
    "\n",
    "# opening pickle\n",
    "infile = open('prediction_direction','rb')\n",
    "new_dict = pickle.load(infile)\n",
    "infile.close()\n",
    "\n",
    "\n",
    "\n",
    "##########Logistic Regression################\n",
    "lag_factors = pd.read_csv('stepwise_factors.csv') #TO CHANGE\n",
    "\n",
    "prediction_proba = dict.fromkeys(top10_pairs)\n",
    "\n",
    "for i in range(len(prediction_proba)):\n",
    "    print(i)\n",
    "    if i == 5:\n",
    "        continue\n",
    "    else:\n",
    "        \n",
    "        pair_name = top10_pairs[i]\n",
    "        print(pair_name)\n",
    "        pair_names = pair_name.split('_')\n",
    "\n",
    "        index_level = price_data[pair_names[0]] / price_data[pair_names[1]]\n",
    "        index_level = index_level.loc[macro_factors.index] # aligns the y data with the x data\n",
    "        index_level = index_level.dropna()\n",
    "\n",
    "        temp_macro = macro_factors.loc[index_level.index]\n",
    "        temp_feedstock = feedstock_factors.loc[index_level.index]\n",
    "\n",
    "        temp_macro = temp_macro.fillna(0)\n",
    "        temp_feedstock = temp_feedstock.fillna(0)\n",
    "\n",
    "        nameP = top10_pairs[i]\n",
    "        row_num = np.where(nameP == lag_factors.pair)[0][0]\n",
    "        lag_factor_names = lag_factors.loc[row_num,['factor_1','factor_2','factor_3','factor_4','factor_5']]\n",
    "        lag_factor_times = lag_factors.loc[row_num,['lag_1','lag_2','lag_3','lag_4','lag_5']]\n",
    "        days_lag = lag_factor_times * 21 # converting to days equivalent\n",
    "        # spec_dates = [input_date - dt.timedelta(days = int(x)) for x in days_lag.tolist()]\n",
    "        # lag_dates = spec_dates\n",
    "\n",
    "\n",
    "\n",
    "        X_vars = pd.DataFrame(np.NaN, index = index_level.index, columns = lag_factor_names.tolist())\n",
    "\n",
    "        # reading in the time series and lagging it by that much\n",
    "        for z in range(len(lag_factor_names)):\n",
    "            if lag_factor_names[z] in temp_macro.columns:\n",
    "                temp = temp_macro.loc[:,lag_factor_names[z]].copy()\n",
    "                temp = temp.shift(int(days_lag[z])) # takes care of the lag\n",
    "                X_vars.loc[:,lag_factor_names[z]] = temp\n",
    "            else:\n",
    "                temp = temp_feedstock.loc[:,lag_factor_names[z]].copy()\n",
    "                temp = temp.shift(int(days_lag[z])) # takes care of the lag\n",
    "                X_vars.loc[:,lag_factor_names[z]] = temp\n",
    "\n",
    "        X_vars = X_vars.dropna(axis = 0)\n",
    "\n",
    "        Y_var = (index_level.shift(periods = -21*6) - index_level)/index_level # 6 months forward\n",
    "\n",
    "        joint_df = X_vars.copy()\n",
    "        joint_df['PX_LAST'] = Y_var[X_vars.index]\n",
    "        last_day_X = joint_df.iloc[-1,:].drop('PX_LAST', axis = 0)\n",
    "        joint_df = joint_df.dropna()\n",
    "        features = joint_df.copy()\n",
    "\n",
    "        joint_df['Direction'] = [max(0,x) for x in np.sign(joint_df['PX_LAST'])]\n",
    "\n",
    "        logit = LogisticRegression()\n",
    "\n",
    "        logit.fit(joint_df.iloc[:,~joint_df.columns.isin([\"Direction\",\"PX_LAST\"])], np.array(joint_df.loc[:,[\"Direction\"]]).ravel())\n",
    "\n",
    "        proba = logit.predict_proba(pd.DataFrame(last_day_X).transpose())[0][1]\n",
    "\n",
    "        prediction_proba[pair_name] = np.round(proba,4)\n",
    "\n",
    "\n",
    "filename = 'prediction_probability'\n",
    "outfile = open(filename, 'wb')\n",
    "pickle.dump(prediction_proba, outfile)\n",
    "outfile.close()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
